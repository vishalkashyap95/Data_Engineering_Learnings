{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs\n",
    "There are 3 ways to created RDDs<br>\n",
    "1 - Parallelized collections<br>\n",
    "2 - External Sources - AWS S3, HDFS, Hive, txt, csv etc<br>\n",
    "3 - From existing RDDs, by transforming the existing RDDs and it returns the data in RDD type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric RDD [10, 20, 30, 40, 50, 60, 70, 10, 90, 80, 40]\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "string RDD ['Apache', 'pyspark', 'python', 'java', 'pop', 'fan', 'bottle']\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# 1st way, using the paralleize collection\n",
    "normal_py_num_list = [10,20,30,40,50,60,70,10,90,80,40]\n",
    "paralleize_num_rdd = sc.parallelize(normal_py_num_list)\n",
    "print(\"Numeric RDD {}\".format(paralleize_num_rdd.collect()))\n",
    "print(type(paralleize_num_rdd))\n",
    "#-----------------------------------------------------------------#\n",
    "normal_py_string_list = ['Apache','pyspark','python','java','pop','fan','bottle']\n",
    "paralleize_string_rdd = sc.parallelize(normal_py_string_list)\n",
    "print(\"string RDD {}\".format(paralleize_string_rdd.collect()))\n",
    "print(type(paralleize_string_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Week 1: (Make Note on google drive and repo in bitbucket for source code)',\n",
       " 'What is Big Data',\n",
       " 'What is role of Data Engineer',\n",
       " 'What Spark and Why Spark',\n",
       " 'PySpark - https://www.tutorialspoint.com/pyspark/index.htm (This is minimum you have to go through. You can even go through any other tutorials on youtube to get understanding of pyspark and Spark) ',\n",
       " 'Understanding Spark architecture and  try to  relate to what you have learned.',\n",
       " 'Some practice running spark programs ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd way to create RDD, using external sources.\n",
    "# sc.addFile(\"D:\\\\DataEngineering_Learnings\\\\Week_1_Task\\\\Week_1_task_requirements.txt\")\n",
    "input_txt_file = sc.textFile(\"D:\\\\DataEngineering_Learnings\\\\Week_1_Task\\\\Week_1_task_requirements.txt\")\n",
    "input_txt_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What Spark and Why Spark',\n",
       " 'PySpark - https://www.tutorialspoint.com/pyspark/index.htm (This is minimum you have to go through. You can even go through any other tutorials on youtube to get understanding of pyspark and Spark) ',\n",
       " 'Understanding Spark architecture and  try to  relate to what you have learned.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd way to create RDD is via some tranformation on RDD which return RDD type of object\n",
    "# Just filter the line which contains Spark keyword\n",
    "new_transformed_rdd = input_txt_file.filter(lambda x: 'Spark' in x)\n",
    "new_transformed_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on RDDs\n",
    "1 - Action<br>\n",
    "2 - Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Data using collect() action -\n",
      " ['Week 1: (Make Note on google drive and repo in bitbucket for source code)', 'What is Big Data', 'What is role of Data Engineer', 'What Spark and Why Spark', 'PySpark - https://www.tutorialspoint.com/pyspark/index.htm (This is minimum you have to go through. You can even go through any other tutorials on youtube to get understanding of pyspark and Spark) ', 'Understanding Spark architecture and  try to  relate to what you have learned.', 'Some practice running spark programs ']\n",
      "\n",
      "Fetching data using take() action -\n",
      " ['Week 1: (Make Note on google drive and repo in bitbucket for source code)', 'What is Big Data']\n",
      "\n",
      "Count of lines - \n",
      " 7\n",
      "\n",
      "First line from file - \n",
      " Week 1: (Make Note on google drive and repo in bitbucket for source code)\n",
      "\n",
      "Reduce method example - \n",
      " 15\n"
     ]
    }
   ],
   "source": [
    "# Action methods\n",
    "\n",
    "# Collect() - Use to retrive the data from all the worker node to driver program, it returns a list \n",
    "print(\"Fetching Data using collect() action -\\n\",input_txt_file.collect())\n",
    "\n",
    "# take() - This method is also use to retrive n number of the data, it returns a list \n",
    "print(\"\\nFetching data using take() action -\\n\",input_txt_file.take(2))\n",
    "\n",
    "# Count() - This method is used to check the lenght of rdd\n",
    "print(\"\\nCount of lines - \\n\",input_txt_file.count())\n",
    "\n",
    "# First() - This method is used to check the first element from the rdd\n",
    "print(\"\\nFirst line from file - \\n\",input_txt_file.first())\n",
    "\n",
    "# reduce() - Use to perform action on new element based on previous calculated element\n",
    "# for eg: sum of [1,2,3,4,5] = 15, we can achive this by reduce method\n",
    "l1 = [1,2,3,4,5]\n",
    "print(\"\\nReduce method example - \\n\",sc.parallelize(l1).reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Week', '1:', '(Make', 'Note', 'on', 'google', 'drive', 'and', 'repo', 'in', 'bitbucket', 'for', 'source', 'code)'], ['What', 'is', 'Big', 'Data'], ['What', 'is', 'role', 'of', 'Data', 'Engineer'], ['What', 'Spark', 'and', 'Why', 'Spark'], ['PySpark', '-', 'https://www.tutorialspoint.com/pyspark/index.htm', '(This', 'is', 'minimum', 'you', 'have', 'to', 'go', 'through.', 'You', 'can', 'even', 'go', 'through', 'any', 'other', 'tutorials', 'on', 'youtube', 'to', 'get', 'understanding', 'of', 'pyspark', 'and', 'Spark)'], ['Understanding', 'Spark', 'architecture', 'and', 'try', 'to', 'relate', 'to', 'what', 'you', 'have', 'learned.'], ['Some', 'practice', 'running', 'spark', 'programs']]\n",
      "\n",
      "Flattened array using flaptmap() - \n",
      " ['Week', '1:', '(Make', 'Note', 'on', 'google', 'drive', 'and', 'repo', 'in']\n",
      "\n",
      "Filtered stop words using Filter() - \n",
      " ['Week', '1:', '(Make', 'Note', 'google', 'drive', 'repo', 'bitbucket', 'source', 'code)', 'What', 'is', 'Big', 'Data', 'What', 'is', 'role', 'of', 'Data', 'Engineer', 'What', 'Spark', 'Why', 'Spark', 'PySpark', '-', 'https://www.tutorialspoint.com/pyspark/index.htm', '(This', 'is', 'minimum', 'have', 'through.', 'You', 'can', 'even', 'through', 'any', 'other', 'tutorials', 'youtube', 'get', 'understanding', 'of', 'pyspark', 'Spark)', 'Understanding', 'Spark', 'architecture', 'try', 'relate', 'what', 'have', 'learned.', 'Some', 'practice', 'running', 'spark', 'programs']\n",
      "\n",
      "Filter words starts with 'S' using Filter() - \n",
      " ['Week', '1:', '(Make', 'Note', 'google', 'drive', 'repo', 'bitbucket', 'source', 'code)', 'What', 'is', 'Big', 'Data', 'What', 'is', 'role', 'of', 'Data', 'Engineer', 'What', 'Spark', 'Why', 'Spark', 'PySpark', '-', 'https://www.tutorialspoint.com/pyspark/index.htm', '(This', 'is', 'minimum', 'have', 'through.', 'You', 'can', 'even', 'through', 'any', 'other', 'tutorials', 'youtube', 'get', 'understanding', 'of', 'pyspark', 'Spark)', 'Understanding', 'Spark', 'architecture', 'try', 'relate', 'what', 'have', 'learned.', 'Some', 'practice', 'running', 'spark', 'programs']\n",
      "\n",
      "[(3, 'What'), (3, 'is'), (3, 'Spark'), (2, 'of'), (2, 'have'), (2, 'Data'), (1, 'Week'), (1, '1:'), (1, '(Make'), (1, 'Note')]\n",
      "\n",
      "Unique Word count without stop words -  56\n",
      "\n",
      "Unique Word count after removing stop words -  49\n"
     ]
    }
   ],
   "source": [
    "# Transformation methods\n",
    "\n",
    "# map() - Use to apply function on each element of rdd. In the below example, entire input file is splitted using space\n",
    "def split_lines(lines):\n",
    "    return lines.split()\n",
    "splitted_rdd = input_txt_file.map(split_lines)\n",
    "print(splitted_rdd.collect())\n",
    "\n",
    "# flatmap() - it is use to flatten the rdd into 1D rdd\n",
    "flatten_array = input_txt_file.flatMap(split_lines)\n",
    "print(\"\\nFlattened array using flaptmap() - \\n\",flatten_array.take(10))\n",
    "\n",
    "# Filter() - This method is use to filter the rdd\n",
    "stop_word = ['a','an','and','the','with','on','to','why','go','in','for','you']\n",
    "filtered_words = input_txt_file.flatMap(split_lines).filter(lambda x: x if x not in stop_word else \"\")\n",
    "print(\"\\nFiltered stop words using Filter() - \\n\",filtered_words.collect())\n",
    "\n",
    "# Filter() - This method is use to filter the rdd\n",
    "filtered_words1 = input_txt_file.flatMap(split_lines).filter(lambda x: x.startswith('S'))\n",
    "print(\"\\nFilter words starts with 'S' using Filter() - \\n\",filtered_words.collect())\n",
    "print()\n",
    "mapped_rdd = filtered_words.map(lambda x: (x,1))\n",
    "grouped_rdd = mapped_rdd.groupByKey()\n",
    "word_count = grouped_rdd.mapValues(sum).map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "print(word_count.take(10))\n",
    "\n",
    "# Distinct()\n",
    "print(\"\\nUnique Word count without stop words - \",flatten_array.distinct().count())\n",
    "print(\"\\nUnique Word count after removing stop words - \",filtered_words.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 4)), ('b', (10, 20))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a',2),('b',10)])\n",
    "rdd2 = sc.parallelize([('a',4),('b',20),('c',30)])\n",
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.csv(\"titanic/train.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Survived|   Sex|\n",
      "+--------+------+\n",
      "|       0|  male|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       0|female|\n",
      "|       1|female|\n",
      "|       0|  male|\n",
      "|       1|  male|\n",
      "|       0|female|\n",
      "|       1|female|\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.select('Survived','Sex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|               714|\n",
      "|   mean| 29.69911764705882|\n",
      "| stddev|14.526497332334035|\n",
      "|    min|              0.42|\n",
      "|    max|              80.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|    Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-----+--------+\n",
      "|         31|       0|     1|Uruchurtu, Don. M...|  male|40.0|    0|    0|  PC 17601| 27.7208| null|       C|\n",
      "|         41|       0|     3|Ahlin, Mrs. Johan...|female|40.0|    1|    0|      7546|   9.475| null|       S|\n",
      "|        162|       1|     2|\"Watt, Mrs. James...|female|40.0|    0|    0|C.A. 33595|   15.75| null|       S|\n",
      "|        189|       0|     3|    Bourke, Mr. John|  male|40.0|    1|    1|    364849|    15.5| null|       Q|\n",
      "|        210|       1|     1|    Blank, Mr. Henry|  male|40.0|    0|    0|    112277|    31.0|  A31|       C|\n",
      "|        264|       0|     1|Harrison, Mr. Wil...|  male|40.0|    0|    0|    112059|     0.0|  B94|       S|\n",
      "|        320|       1|     1|Spedden, Mrs. Fre...|female|40.0|    1|    1|     16966|   134.5|  E34|       C|\n",
      "|        347|       1|     2|Smith, Miss. Mari...|female|40.0|    0|    0|     31418|    13.0| null|       S|\n",
      "|        361|       0|     3|  Skoog, Mr. Wilhelm|  male|40.0|    1|    4|    347088|    27.9| null|       S|\n",
      "|        562|       0|     3|   Sivic, Mr. Husein|  male|40.0|    0|    0|    349251|  7.8958| null|       S|\n",
      "|        610|       1|     1|Shutes, Miss. Eli...|female|40.0|    0|    0|  PC 17582|153.4625| C125|       S|\n",
      "|        662|       0|     3|   Badt, Mr. Mohamed|  male|40.0|    0|    0|      2623|   7.225| null|       C|\n",
      "|        671|       1|     2|Brown, Mrs. Thoma...|female|40.0|    1|    1|     29750|    39.0| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.filter(titanic_df.Age==40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|    Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|  113783|   26.55| C103|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|  248706|    16.0| null|       S|\n",
      "|         53|       1|     1|Harper, Mrs. Henr...|female|49.0|    1|    0|PC 17572| 76.7292|  D33|       C|\n",
      "|        188|       1|     1|\"Romaine, Mr. Cha...|  male|45.0|    0|    0|  111428|   26.55| null|       S|\n",
      "|        195|       1|     1|Brown, Mrs. James...|female|44.0|    0|    0|PC 17610| 27.7208|   B4|       C|\n",
      "|        196|       1|     1|Lurette, Miss. Elise|female|58.0|    0|    0|PC 17569|146.5208|  B80|       C|\n",
      "|        260|       1|     2|Parrish, Mrs. (Lu...|female|50.0|    0|    1|  230433|    26.0| null|       S|\n",
      "|        269|       1|     1|Graham, Mrs. Will...|female|58.0|    0|    1|PC 17582|153.4625| C125|       S|\n",
      "|        273|       1|     2|Mellinger, Mrs. (...|female|41.0|    0|    1|  250644|    19.5| null|       S|\n",
      "|        276|       1|     1|Andrews, Miss. Ko...|female|63.0|    1|    0|   13502| 77.9583|   D7|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.where((titanic_df.Age > 40) & (titanic_df.Survived==1)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating temp table, so that we can query like SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the df to table with the name titanic_sql\n",
    "titanic_df.registerTempTable(\"titanic_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from titanic_sql\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|    Fare|  Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-------+--------+\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|    113783|   26.55|   C103|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|    248706|    16.0|   null|       S|\n",
      "|         53|       1|     1|Harper, Mrs. Henr...|female|49.0|    1|    0|  PC 17572| 76.7292|    D33|       C|\n",
      "|        162|       1|     2|\"Watt, Mrs. James...|female|40.0|    0|    0|C.A. 33595|   15.75|   null|       S|\n",
      "|        188|       1|     1|\"Romaine, Mr. Cha...|  male|45.0|    0|    0|    111428|   26.55|   null|       S|\n",
      "|        195|       1|     1|Brown, Mrs. James...|female|44.0|    0|    0|  PC 17610| 27.7208|     B4|       C|\n",
      "|        196|       1|     1|Lurette, Miss. Elise|female|58.0|    0|    0|  PC 17569|146.5208|    B80|       C|\n",
      "|        210|       1|     1|    Blank, Mr. Henry|  male|40.0|    0|    0|    112277|    31.0|    A31|       C|\n",
      "|        260|       1|     2|Parrish, Mrs. (Lu...|female|50.0|    0|    1|    230433|    26.0|   null|       S|\n",
      "|        269|       1|     1|Graham, Mrs. Will...|female|58.0|    0|    1|  PC 17582|153.4625|   C125|       S|\n",
      "|        273|       1|     2|Mellinger, Mrs. (...|female|41.0|    0|    1|    250644|    19.5|   null|       S|\n",
      "|        276|       1|     1|Andrews, Miss. Ko...|female|63.0|    1|    0|     13502| 77.9583|     D7|       S|\n",
      "|        289|       1|     2|Hosono, Mr. Masabumi|  male|42.0|    0|    0|    237798|    13.0|   null|       S|\n",
      "|        300|       1|     1|Baxter, Mrs. Jame...|female|50.0|    0|    1|  PC 17558|247.5208|B58 B60|       C|\n",
      "|        320|       1|     1|Spedden, Mrs. Fre...|female|40.0|    1|    1|     16966|   134.5|    E34|       C|\n",
      "|        338|       1|     1|Burns, Miss. Eliz...|female|41.0|    0|    0|     16966|   134.5|    E40|       C|\n",
      "|        339|       1|     3|Dahl, Mr. Karl Ed...|  male|45.0|    0|    0|      7598|    8.05|   null|       S|\n",
      "|        347|       1|     2|Smith, Miss. Mari...|female|40.0|    0|    0|     31418|    13.0|   null|       S|\n",
      "|        367|       1|     1|Warren, Mrs. Fran...|female|60.0|    1|    0|    110813|   75.25|    D37|       C|\n",
      "|        381|       1|     1|Bidois, Miss. Ros...|female|42.0|    0|    0|  PC 17757| 227.525|   null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from titanic_sql where Age >= 40 and Survived=1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      61|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(*) from titanic_sql where Age >= 40 and Survived=1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to postgresql db and reading/writing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - Launched pyspark with below command\n",
    "# pyspark --driver-class-path .\\postgresql-42.2.18.jar --jars .\\postgresql-42.2.18.jar\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', \"postgresql-42.2.18.jar\").getOrCreate()\n",
    "url = 'jdbc:postgresql://127.0.0.1/postgres'\n",
    "properties = {'user': 'postgres', 'password': 'n0ob007'}\n",
    "df = spark.read.jdbc(url=url, table='active_users', properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+----------+-----+------+\n",
      "|user_id|country|     platform|  date_utc|years|months|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "|      1|Android|           PE|01-07-2020| 2020|     7|\n",
      "|      2|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      3|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      4|Android|           PT|01-07-2020| 2020|     7|\n",
      "|      5|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      6|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      7|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      8|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      9|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     10|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     11|Android|           US|01-07-2020| 2020|     7|\n",
      "|     12|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     13|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     14|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     15|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     16|Android|           US|01-07-2020| 2020|     7|\n",
      "|     17|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     18|Android|           US|01-07-2020| 2020|     7|\n",
      "|     19|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     20|    iOS|United States|01-07-2020| 2020|     7|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- date_utc: string (nullable = true)\n",
      " |-- years: integer (nullable = true)\n",
      " |-- months: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SparkSession'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ef52b3418092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicits\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m a = spark.createDataset(\"\"\"{\"user_id\":150,\n\u001b[0;32m      3\u001b[0m        \u001b[1;34m\"country\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"Android\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m        \u001b[1;34m\"platform\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"United States\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;34m\"date_utc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"01-07-2020\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SparkSession'"
     ]
    }
   ],
   "source": [
    "from SparkSession.implicits import *\n",
    "a = spark.createDataset(\"\"\"{\"user_id\":150,\n",
    "       \"country\":\"Android\",\n",
    "       \"platform\":\"United States\",\n",
    "       \"date_utc\":\"01-07-2020\",\n",
    "       \"years\":2020,\n",
    "       \"months\":2}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_df = spark.read.json(\"test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_df.registerTempTable(\"json_data_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_data_sql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-15dcb0406103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjson_data_sql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json_data_sql' is not defined"
     ]
    }
   ],
   "source": [
    "json_data_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframe in pandas and converting it to pyspark dataframe and insert it to postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'user_id':[150],\n",
    "       'country':[\"Android\"],\n",
    "       'platform':['United States'],\n",
    "       'date_utc':['01-07-2020'],\n",
    "       'years':[2020],\n",
    "       'months':[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+----------+-----+------+\n",
      "|user_id|country|     platform|  date_utc|years|months|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "|    150|Android|United States|01-07-2020| 2020|     2|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pyspark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df.write\\\n",
    "        .format(\"jdbc\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"truncate\",\"true\")\\\n",
    "        .option(\"url\",url)\\\n",
    "        .option(\"dbtable\",\"active_users\")\\\n",
    "        .option(\"user\",\"postgres\")\\\n",
    "        .option(\"password\",\"n0ob007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:postgresql://127.0.0.1/postgres'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 - Week 2\n",
    "### Create a pyspark script which will read data from particular table from Mysql/Postgres database\n",
    "### Perform some operations/transformation on the data and save the result to local file storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - Launched pyspark with below command\n",
    "# pyspark --driver-class-path .\\postgresql-42.2.18.jar --jars .\\postgresql-42.2.18.jar\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', \"postgresql-42.2.18.jar\").getOrCreate()\n",
    "url = 'jdbc:postgresql://127.0.0.1/postgres'\n",
    "properties = {'user': 'postgres', 'password': 'n0ob007'}\n",
    "emp_df = spark.read.jdbc(url=url, table='Emp', properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+\n",
      "|empname|   empdesignation|emplocation|\n",
      "+-------+-----------------+-----------+\n",
      "|   John|Software Engineer| California|\n",
      "|   Wade|    Data Engineer|    Toronto|\n",
      "| Vishal|    Data Engineer|     Mumbai|\n",
      "|   Adam|    Test Engineer|   New York|\n",
      "+-------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Dataframe into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empname='John', empdesignation='Software Engineer', emplocation='California'),\n",
       " Row(empname='Wade', empdesignation='Data Engineer', emplocation='Toronto'),\n",
       " Row(empname='Vishal', empdesignation='Data Engineer', emplocation='Mumbai'),\n",
       " Row(empname='Adam', empdesignation='Test Engineer', emplocation='New York')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = sc.parallelize(emp_df.rdd.map(lambda row: row.asDict()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'empname': 'John',\n",
       "  'empdesignation': 'Software Engineer',\n",
       "  'emplocation': 'California'},\n",
       " {'empname': 'Wade',\n",
       "  'empdesignation': 'Data Engineer',\n",
       "  'emplocation': 'Toronto'},\n",
       " {'empname': 'Vishal',\n",
       "  'empdesignation': 'Data Engineer',\n",
       "  'emplocation': 'Mumbai'},\n",
       " {'empname': 'Adam',\n",
       "  'empdesignation': 'Test Engineer',\n",
       "  'emplocation': 'New York'}]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Software Engineer'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.collect()[0]['empdesignation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to convert Dataframe into RDD and then convert RDD into list of dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'empname': 'John',\n",
       "  'empdesignation': 'Software Engineer',\n",
       "  'emplocation': 'California'},\n",
       " {'empname': 'Wade',\n",
       "  'empdesignation': 'Data Engineer',\n",
       "  'emplocation': 'Toronto'},\n",
       " {'empname': 'Vishal',\n",
       "  'empdesignation': 'Data Engineer',\n",
       "  'emplocation': 'Mumbai'},\n",
       " {'empname': 'Adam',\n",
       "  'empdesignation': 'Test Engineer',\n",
       "  'emplocation': 'New York'}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.rdd.map(lambda row: row.asDict()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new column and transforming column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# new_emp_df = emp_df.withColumnRenamed('emplocation','emp_location')\n",
    "# new_emp_df = new_emp_df.withColumnRenamed('empdesignation','emp_designation')\n",
    "# new_emp_df = new_emp_df.withColumnRenamed('empname','emp_name')\n",
    "new_emp_df = emp_df.select(\n",
    "    F.col('emplocation').alias('emp_location'),F.col('empdesignation').alias('emp_designation'),F.col('empname').alias('emp_name')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------+\n",
      "|emp_location|  emp_designation|emp_name|\n",
      "+------------+-----------------+--------+\n",
      "|  California|Software Engineer|    John|\n",
      "|     Toronto|    Data Engineer|    Wade|\n",
      "|      Mumbai|    Data Engineer|  Vishal|\n",
      "|    New York|    Test Engineer|    Adam|\n",
      "+------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lit\n",
    "# new_emp_df = new_emp_df.withColumn('emp_desig_sf',lit('NaN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_emp_df.withColumn(\"emp_desig_sf\",when(col(\"emp_designation\") == \"Software Engineer\",\"SE\")\n",
    "#                       .when(col(\"emp_designation\") == \"Data Engineer\",\"DE\")\n",
    "#                       .when(col(\"emp_designation\") == \"Test Engineer\")\n",
    "#                       .otherwise(\"Unknown\"))\n",
    "\n",
    "# df.withColumn(\"new_gender\", when(col(\"gender\") === \"M\",\"Male\")\n",
    "#       .when(col(\"gender\") === \"F\",\"Female\")\n",
    "#       .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_initials(col):\n",
    "    splitted_data = str(col).strip().split()\n",
    "    return ''.join([str(i[0]).upper() for i in splitted_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_initials(\"Data Engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------+------------+\n",
      "|emp_location|  emp_designation|emp_name|emp_desig_sf|\n",
      "+------------+-----------------+--------+------------+\n",
      "|  California|Software Engineer|    John|          SE|\n",
      "|     Toronto|    Data Engineer|    Wade|          DE|\n",
      "|      Mumbai|    Data Engineer|  Vishal|          DE|\n",
      "|    New York|    Test Engineer|    Adam|          TE|\n",
      "+------------+-----------------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sqlFunc\n",
    "import pyspark.sql.types as sqlTypes\n",
    "usr_def_func = sqlFunc.UserDefinedFunction(extract_initials,sqlTypes.StringType())\n",
    "transformed_emp_df = new_emp_df.withColumn('emp_desig_sf',usr_def_func('emp_designation'))\n",
    "transformed_emp_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_emp_df.toPandas().to_csv(\"Transformed_emp_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 - Week 3\n",
    "### MoviLens dataset\n",
    "### Dataset url:https://grouplens.org/datasets/movielens/\n",
    "### Use PySpark for the followings\n",
    "### Find the Most Popular Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "links_df = pd.read_csv(\"./ml-latest-small/links.csv\")\n",
    "movies_df = pd.read_csv(\"./ml-latest-small/movies.csv\")\n",
    "ratings_df = pd.read_csv(\"./ml-latest-small/ratings.csv\")\n",
    "tags_df = pd.read_csv(\"./ml-latest-small/tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>113497</td>\n",
       "      <td>8844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>113228</td>\n",
       "      <td>15602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>114885</td>\n",
       "      <td>31357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>113041</td>\n",
       "      <td>11862.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  imdbId   tmdbId\n",
       "0        1  114709    862.0\n",
       "1        2  113497   8844.0\n",
       "2        3  113228  15602.0\n",
       "3        4  114885  31357.0\n",
       "4        5  113041  11862.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>60756</td>\n",
       "      <td>funny</td>\n",
       "      <td>1445714994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60756</td>\n",
       "      <td>Highly quotable</td>\n",
       "      <td>1445714996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60756</td>\n",
       "      <td>will ferrell</td>\n",
       "      <td>1445714992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>89774</td>\n",
       "      <td>Boxing story</td>\n",
       "      <td>1445715207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>89774</td>\n",
       "      <td>MMA</td>\n",
       "      <td>1445715200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId              tag   timestamp\n",
       "0       2    60756            funny  1445714994\n",
       "1       2    60756  Highly quotable  1445714996\n",
       "2       2    60756     will ferrell  1445714992\n",
       "3       2    89774     Boxing story  1445715207\n",
       "4       2    89774              MMA  1445715200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 most Rated movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 2 dataframe (movies and ratings)\n",
    "movies_ratings = pd.merge(movies_df,ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>847434962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1106635946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1510577970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1305696483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId             title                                       genres  \\\n",
       "0        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
       "1        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
       "2        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
       "3        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
       "4        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
       "\n",
       "   userId  rating   timestamp  \n",
       "0       1     4.0   964982703  \n",
       "1       5     4.0   847434962  \n",
       "2       7     4.5  1106635946  \n",
       "3      15     2.5  1510577970  \n",
       "4      17     4.5  1305696483  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANDAS : First way to find most rated movies by using Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Forrest Gump (1994)</th>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shawshank Redemption, The (1994)</th>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulp Fiction (1994)</th>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Silence of the Lambs, The (1991)</th>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matrix, The (1999)</th>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star Wars: Episode IV - A New Hope (1977)</th>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jurassic Park (1993)</th>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Braveheart (1995)</th>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminator 2: Judgment Day (1991)</th>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schindler's List (1993)</th>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fight Club (1999)</th>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toy Story (1995)</th>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star Wars: Episode V - The Empire Strikes Back (1980)</th>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Usual Suspects, The (1995)</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Beauty (1999)</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seven (a.k.a. Se7en) (1995)</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Independence Day (a.k.a. ID4) (1996)</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apollo 13 (1995)</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord of the Rings: The Fellowship of the Ring, The (2001)</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star Wars: Episode VI - Return of the Jedi (1983)</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Godfather, The (1972)</th>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fugitive, The (1993)</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Batman (1989)</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saving Private Ryan (1998)</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Count\n",
       "title                                                    \n",
       "Forrest Gump (1994)                                   329\n",
       "Shawshank Redemption, The (1994)                      317\n",
       "Pulp Fiction (1994)                                   307\n",
       "Silence of the Lambs, The (1991)                      279\n",
       "Matrix, The (1999)                                    278\n",
       "Star Wars: Episode IV - A New Hope (1977)             251\n",
       "Jurassic Park (1993)                                  238\n",
       "Braveheart (1995)                                     237\n",
       "Terminator 2: Judgment Day (1991)                     224\n",
       "Schindler's List (1993)                               220\n",
       "Fight Club (1999)                                     218\n",
       "Toy Story (1995)                                      215\n",
       "Star Wars: Episode V - The Empire Strikes Back ...    211\n",
       "Usual Suspects, The (1995)                            204\n",
       "American Beauty (1999)                                204\n",
       "Seven (a.k.a. Se7en) (1995)                           203\n",
       "Independence Day (a.k.a. ID4) (1996)                  202\n",
       "Apollo 13 (1995)                                      201\n",
       "Raiders of the Lost Ark (Indiana Jones and the ...    200\n",
       "Lord of the Rings: The Fellowship of the Ring, ...    198\n",
       "Star Wars: Episode VI - Return of the Jedi (1983)     196\n",
       "Godfather, The (1972)                                 192\n",
       "Fugitive, The (1993)                                  190\n",
       "Batman (1989)                                         189\n",
       "Saving Private Ryan (1998)                            188"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_ratings.groupby('title').size().to_frame().rename(columns={0:'Count'}).sort_values(by='Count',ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANDAS : 2nd way to find most rated movies by values_count() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Forrest Gump (1994)                                                               329\n",
       "Shawshank Redemption, The (1994)                                                  317\n",
       "Pulp Fiction (1994)                                                               307\n",
       "Silence of the Lambs, The (1991)                                                  279\n",
       "Matrix, The (1999)                                                                278\n",
       "Star Wars: Episode IV - A New Hope (1977)                                         251\n",
       "Jurassic Park (1993)                                                              238\n",
       "Braveheart (1995)                                                                 237\n",
       "Terminator 2: Judgment Day (1991)                                                 224\n",
       "Schindler's List (1993)                                                           220\n",
       "Fight Club (1999)                                                                 218\n",
       "Toy Story (1995)                                                                  215\n",
       "Star Wars: Episode V - The Empire Strikes Back (1980)                             211\n",
       "Usual Suspects, The (1995)                                                        204\n",
       "American Beauty (1999)                                                            204\n",
       "Seven (a.k.a. Se7en) (1995)                                                       203\n",
       "Independence Day (a.k.a. ID4) (1996)                                              202\n",
       "Apollo 13 (1995)                                                                  201\n",
       "Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)    200\n",
       "Lord of the Rings: The Fellowship of the Ring, The (2001)                         198\n",
       "Star Wars: Episode VI - Return of the Jedi (1983)                                 196\n",
       "Godfather, The (1972)                                                             192\n",
       "Fugitive, The (1993)                                                              190\n",
       "Batman (1989)                                                                     189\n",
       "Saving Private Ryan (1998)                                                        188\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_ratings['title'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Pyspark\n",
    "### 1 : Read both the csv(movies.csv and ratings.csv)\n",
    "### 2 : Merge both the csv using .join method\n",
    "### 3 : converting dataframe into sql temp table and query via sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_movies_df = spark.read.csv(\"./ml-latest-small/movies.csv\",header=True)\n",
    "spark_ratings_df = spark.read.csv(\"./ml-latest-small/ratings.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_movies_df.show(5)\n",
    "spark_ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_movies_ratings_df = spark_movies_df.join(spark_ratings_df,on=['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+------+---------+\n",
      "|movieId|               title|              genres|userId|rating|timestamp|\n",
      "+-------+--------------------+--------------------+------+------+---------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|     1|   4.0|964982703|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|     1|   4.0|964981247|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|     1|   4.0|964982224|\n",
      "|     47|Seven (a.k.a. Se7...|    Mystery|Thriller|     1|   5.0|964983815|\n",
      "|     50|Usual Suspects, T...|Crime|Mystery|Thr...|     1|   5.0|964982931|\n",
      "|     70|From Dusk Till Da...|Action|Comedy|Hor...|     1|   3.0|964982400|\n",
      "|    101|Bottle Rocket (1996)|Adventure|Comedy|...|     1|   5.0|964980868|\n",
      "|    110|   Braveheart (1995)|    Action|Drama|War|     1|   4.0|964982176|\n",
      "|    151|      Rob Roy (1995)|Action|Drama|Roma...|     1|   5.0|964984041|\n",
      "|    157|Canadian Bacon (1...|          Comedy|War|     1|   5.0|964984100|\n",
      "+-------+--------------------+--------------------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_movies_ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 way, via merging df and group by on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|count|\n",
      "+--------------------+-----+\n",
      "| Forrest Gump (1994)|  329|\n",
      "|Shawshank Redempt...|  317|\n",
      "| Pulp Fiction (1994)|  307|\n",
      "|Silence of the La...|  279|\n",
      "|  Matrix, The (1999)|  278|\n",
      "|Star Wars: Episod...|  251|\n",
      "|Jurassic Park (1993)|  238|\n",
      "|   Braveheart (1995)|  237|\n",
      "|Terminator 2: Jud...|  224|\n",
      "|Schindler's List ...|  220|\n",
      "|   Fight Club (1999)|  218|\n",
      "|    Toy Story (1995)|  215|\n",
      "|Star Wars: Episod...|  211|\n",
      "|Usual Suspects, T...|  204|\n",
      "|American Beauty (...|  204|\n",
      "|Seven (a.k.a. Se7...|  203|\n",
      "|Independence Day ...|  202|\n",
      "|    Apollo 13 (1995)|  201|\n",
      "|Raiders of the Lo...|  200|\n",
      "|Lord of the Rings...|  198|\n",
      "|Star Wars: Episod...|  196|\n",
      "|Godfather, The (1...|  192|\n",
      "|Fugitive, The (1993)|  190|\n",
      "|       Batman (1989)|  189|\n",
      "|Saving Private Ry...|  188|\n",
      "+--------------------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc,col\n",
    "# spark_movies_ratings_df.groupBy('title').count().orderBy('count',ascending=False).show(25)\n",
    "spark_movies_ratings_df.groupBy('title').count().sort(col('count').desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd way via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_movies_ratings_df.registerTempTable(\"movies_ratings_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|               title|count(title)|\n",
      "+--------------------+------------+\n",
      "| Forrest Gump (1994)|         329|\n",
      "|Shawshank Redempt...|         317|\n",
      "| Pulp Fiction (1994)|         307|\n",
      "|Silence of the La...|         279|\n",
      "|  Matrix, The (1999)|         278|\n",
      "|Star Wars: Episod...|         251|\n",
      "|Jurassic Park (1993)|         238|\n",
      "|   Braveheart (1995)|         237|\n",
      "|Terminator 2: Jud...|         224|\n",
      "|Schindler's List ...|         220|\n",
      "|   Fight Club (1999)|         218|\n",
      "|    Toy Story (1995)|         215|\n",
      "|Star Wars: Episod...|         211|\n",
      "|Usual Suspects, T...|         204|\n",
      "|American Beauty (...|         204|\n",
      "|Seven (a.k.a. Se7...|         203|\n",
      "|Independence Day ...|         202|\n",
      "|    Apollo 13 (1995)|         201|\n",
      "|Raiders of the Lo...|         200|\n",
      "|Lord of the Rings...|         198|\n",
      "|Star Wars: Episod...|         196|\n",
      "|Godfather, The (1...|         192|\n",
      "|Fugitive, The (1993)|         190|\n",
      "|       Batman (1989)|         189|\n",
      "|Lord of the Rings...|         188|\n",
      "+--------------------+------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select title,COUNT('title') from movies_ratings_table group by title order by 2 desc\").show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
