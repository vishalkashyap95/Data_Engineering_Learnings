{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divisibleBy2(num):\n",
    "    if num%2==0:\n",
    "        return num\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1685844"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,43,2,3,4,2,62,5,1234,23,4,2,243,34,22,214,32,4,2,234,3,4,34,4,234,243]).map(lambda x:x**2).filter(divisibleBy2).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SparkConf in module pyspark.conf:\n",
      "\n",
      "class SparkConf(builtins.object)\n",
      " |  SparkConf(loadDefaults=True, _jvm=None, _jconf=None)\n",
      " |  \n",
      " |  Configuration for a Spark application. Used to set various Spark\n",
      " |  parameters as key-value pairs.\n",
      " |  \n",
      " |  Most of the time, you would create a SparkConf object with\n",
      " |  ``SparkConf()``, which will load values from `spark.*` Java system\n",
      " |  properties as well. In this case, any parameters you set directly on\n",
      " |  the :class:`SparkConf` object take priority over system properties.\n",
      " |  \n",
      " |  For unit tests, you can also call ``SparkConf(false)`` to skip\n",
      " |  loading external settings and get the same configuration no matter\n",
      " |  what the system properties are.\n",
      " |  \n",
      " |  All setter methods in this class support chaining. For example,\n",
      " |  you can write ``conf.setMaster(\"local\").setAppName(\"My app\")``.\n",
      " |  \n",
      " |  .. note:: Once a SparkConf object is passed to Spark, it is cloned\n",
      " |      and can no longer be modified by the user.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loadDefaults=True, _jvm=None, _jconf=None)\n",
      " |      Create a new Spark configuration.\n",
      " |      \n",
      " |      :param loadDefaults: whether to load values from Java system\n",
      " |             properties (True by default)\n",
      " |      :param _jvm: internal parameter used to pass a handle to the\n",
      " |             Java VM; does not need to be set by users\n",
      " |      :param _jconf: Optionally pass in an existing SparkConf handle\n",
      " |             to use its parameters\n",
      " |  \n",
      " |  contains(self, key)\n",
      " |      Does this configuration contain a given key?\n",
      " |  \n",
      " |  get(self, key, defaultValue=None)\n",
      " |      Get the configured value for some key, or return a default otherwise.\n",
      " |  \n",
      " |  getAll(self)\n",
      " |      Get all values as a list of key-value pairs.\n",
      " |  \n",
      " |  set(self, key, value)\n",
      " |      Set a configuration property.\n",
      " |  \n",
      " |  setAll(self, pairs)\n",
      " |      Set multiple parameters, passed as a list of key-value pairs.\n",
      " |      \n",
      " |      :param pairs: list of key-value pairs to set\n",
      " |  \n",
      " |  setAppName(self, value)\n",
      " |      Set application name.\n",
      " |  \n",
      " |  setExecutorEnv(self, key=None, value=None, pairs=None)\n",
      " |      Set an environment variable to be passed to executors.\n",
      " |  \n",
      " |  setIfMissing(self, key, value)\n",
      " |      Set a configuration property, if not already set.\n",
      " |  \n",
      " |  setMaster(self, value)\n",
      " |      Set master URL to connect to.\n",
      " |  \n",
      " |  setSparkHome(self, value)\n",
      " |      Set path where Spark is installed on worker nodes.\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      Returns a printable version of the configuration, as a list of\n",
      " |      key=value pairs, one per line.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'host.docker.internal'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '64344'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.app.id', 'local-1606051240357'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
