{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs\n",
    "There are 3 ways to created RDDs<br>\n",
    "1 - Parallelized collections<br>\n",
    "2 - External Sources - AWS S3, HDFS, Hive, txt, csv etc<br>\n",
    "3 - From existing RDDs, by transforming the existing RDDs and it returns the data in RDD type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric RDD [10, 20, 30, 40, 50, 60, 70, 10, 90, 80, 40]\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "string RDD ['Apache', 'pyspark', 'python', 'java', 'pop', 'fan', 'bottle']\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# 1st way, using the paralleize collection\n",
    "normal_py_num_list = [10,20,30,40,50,60,70,10,90,80,40]\n",
    "paralleize_num_rdd = sc.parallelize(normal_py_num_list)\n",
    "print(\"Numeric RDD {}\".format(paralleize_num_rdd.collect()))\n",
    "print(type(paralleize_num_rdd))\n",
    "#-----------------------------------------------------------------#\n",
    "normal_py_string_list = ['Apache','pyspark','python','java','pop','fan','bottle']\n",
    "paralleize_string_rdd = sc.parallelize(normal_py_string_list)\n",
    "print(\"string RDD {}\".format(paralleize_string_rdd.collect()))\n",
    "print(type(paralleize_string_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Week 1: (Make Note on google drive and repo in bitbucket for source code)',\n",
       " 'What is Big Data',\n",
       " 'What is role of Data Engineer',\n",
       " 'What Spark and Why Spark',\n",
       " 'PySpark - https://www.tutorialspoint.com/pyspark/index.htm (This is minimum you have to go through. You can even go through any other tutorials on youtube to get understanding of pyspark and Spark) ',\n",
       " 'Understanding Spark architecture and  try to  relate to what you have learned.',\n",
       " 'Some practice running spark programs ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd way to create RDD, using external sources.\n",
    "# sc.addFile(\"D:\\\\DataEngineering_Learnings\\\\Week_1_Task\\\\Week_1_task_requirements.txt\")\n",
    "input_txt_file = sc.textFile(\"D:\\\\DataEngineering_Learnings\\\\Week_1_Task\\\\Week_1_task_requirements.txt\")\n",
    "input_txt_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What Spark and Why Spark',\n",
       " 'PySpark - https://www.tutorialspoint.com/pyspark/index.htm (This is minimum you have to go through. You can even go through any other tutorials on youtube to get understanding of pyspark and Spark) ',\n",
       " 'Understanding Spark architecture and  try to  relate to what you have learned.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd way to create RDD is via some tranformation on RDD which return RDD type of object\n",
    "# Just filter the line which contains Spark keyword\n",
    "new_transformed_rdd = input_txt_file.filter(lambda x: 'Spark' in x)\n",
    "new_transformed_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on RDDs\n",
    "1 - Action<br>\n",
    "2 - Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Data using collect() action -\n",
      " ['Week 1: (Make Note on google drive and repo in bitbucket for source code)', 'What is Big Data', 'What is role of Data Engineer', 'What Spark and Why Spark', 'PySpark - https://www.tutorialspoint.com/pyspark/index.htm (This is minimum you have to go through. You can even go through any other tutorials on youtube to get understanding of pyspark and Spark) ', 'Understanding Spark architecture and  try to  relate to what you have learned.', 'Some practice running spark programs ']\n",
      "\n",
      "Fetching data using take() action -\n",
      " ['Week 1: (Make Note on google drive and repo in bitbucket for source code)', 'What is Big Data']\n",
      "\n",
      "Count of lines - \n",
      " 7\n",
      "\n",
      "First line from file - \n",
      " Week 1: (Make Note on google drive and repo in bitbucket for source code)\n",
      "\n",
      "Reduce method example - \n",
      " 15\n"
     ]
    }
   ],
   "source": [
    "# Action methods\n",
    "\n",
    "# Collect() - Use to retrive the data from all the worker node to driver program, it returns a list \n",
    "print(\"Fetching Data using collect() action -\\n\",input_txt_file.collect())\n",
    "\n",
    "# take() - This method is also use to retrive n number of the data, it returns a list \n",
    "print(\"\\nFetching data using take() action -\\n\",input_txt_file.take(2))\n",
    "\n",
    "# Count() - This method is used to check the lenght of rdd\n",
    "print(\"\\nCount of lines - \\n\",input_txt_file.count())\n",
    "\n",
    "# First() - This method is used to check the first element from the rdd\n",
    "print(\"\\nFirst line from file - \\n\",input_txt_file.first())\n",
    "\n",
    "# reduce() - Use to perform action on new element based on previous calculated element\n",
    "# for eg: sum of [1,2,3,4,5] = 15, we can achive this by reduce method\n",
    "l1 = [1,2,3,4,5]\n",
    "print(\"\\nReduce method example - \\n\",sc.parallelize(l1).reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Week', '1:', '(Make', 'Note', 'on', 'google', 'drive', 'and', 'repo', 'in', 'bitbucket', 'for', 'source', 'code)'], ['What', 'is', 'Big', 'Data'], ['What', 'is', 'role', 'of', 'Data', 'Engineer'], ['What', 'Spark', 'and', 'Why', 'Spark'], ['PySpark', '-', 'https://www.tutorialspoint.com/pyspark/index.htm', '(This', 'is', 'minimum', 'you', 'have', 'to', 'go', 'through.', 'You', 'can', 'even', 'go', 'through', 'any', 'other', 'tutorials', 'on', 'youtube', 'to', 'get', 'understanding', 'of', 'pyspark', 'and', 'Spark)'], ['Understanding', 'Spark', 'architecture', 'and', 'try', 'to', 'relate', 'to', 'what', 'you', 'have', 'learned.'], ['Some', 'practice', 'running', 'spark', 'programs']]\n",
      "\n",
      "Flattened array using flaptmap() - \n",
      " ['Week', '1:', '(Make', 'Note', 'on', 'google', 'drive', 'and', 'repo', 'in']\n",
      "\n",
      "Filtered stop words using Filter() - \n",
      " ['Week', '1:', '(Make', 'Note', 'google', 'drive', 'repo', 'bitbucket', 'source', 'code)', 'What', 'is', 'Big', 'Data', 'What', 'is', 'role', 'of', 'Data', 'Engineer', 'What', 'Spark', 'Why', 'Spark', 'PySpark', '-', 'https://www.tutorialspoint.com/pyspark/index.htm', '(This', 'is', 'minimum', 'have', 'through.', 'You', 'can', 'even', 'through', 'any', 'other', 'tutorials', 'youtube', 'get', 'understanding', 'of', 'pyspark', 'Spark)', 'Understanding', 'Spark', 'architecture', 'try', 'relate', 'what', 'have', 'learned.', 'Some', 'practice', 'running', 'spark', 'programs']\n",
      "\n",
      "Filter words starts with 'S' using Filter() - \n",
      " ['Week', '1:', '(Make', 'Note', 'google', 'drive', 'repo', 'bitbucket', 'source', 'code)', 'What', 'is', 'Big', 'Data', 'What', 'is', 'role', 'of', 'Data', 'Engineer', 'What', 'Spark', 'Why', 'Spark', 'PySpark', '-', 'https://www.tutorialspoint.com/pyspark/index.htm', '(This', 'is', 'minimum', 'have', 'through.', 'You', 'can', 'even', 'through', 'any', 'other', 'tutorials', 'youtube', 'get', 'understanding', 'of', 'pyspark', 'Spark)', 'Understanding', 'Spark', 'architecture', 'try', 'relate', 'what', 'have', 'learned.', 'Some', 'practice', 'running', 'spark', 'programs']\n",
      "\n",
      "[(3, 'What'), (3, 'is'), (3, 'Spark'), (2, 'of'), (2, 'have'), (2, 'Data'), (1, 'Week'), (1, '1:'), (1, '(Make'), (1, 'Note')]\n",
      "\n",
      "Unique Word count without stop words -  56\n",
      "\n",
      "Unique Word count after removing stop words -  49\n"
     ]
    }
   ],
   "source": [
    "# Transformation methods\n",
    "\n",
    "# map() - Use to apply function on each element of rdd. In the below example, entire input file is splitted using space\n",
    "def split_lines(lines):\n",
    "    return lines.split()\n",
    "splitted_rdd = input_txt_file.map(split_lines)\n",
    "print(splitted_rdd.collect())\n",
    "\n",
    "# flatmap() - it is use to flatten the rdd into 1D rdd\n",
    "flatten_array = input_txt_file.flatMap(split_lines)\n",
    "print(\"\\nFlattened array using flaptmap() - \\n\",flatten_array.take(10))\n",
    "\n",
    "# Filter() - This method is use to filter the rdd\n",
    "stop_word = ['a','an','and','the','with','on','to','why','go','in','for','you']\n",
    "filtered_words = input_txt_file.flatMap(split_lines).filter(lambda x: x if x not in stop_word else \"\")\n",
    "print(\"\\nFiltered stop words using Filter() - \\n\",filtered_words.collect())\n",
    "\n",
    "# Filter() - This method is use to filter the rdd\n",
    "filtered_words1 = input_txt_file.flatMap(split_lines).filter(lambda x: x.startswith('S'))\n",
    "print(\"\\nFilter words starts with 'S' using Filter() - \\n\",filtered_words.collect())\n",
    "print()\n",
    "mapped_rdd = filtered_words.map(lambda x: (x,1))\n",
    "grouped_rdd = mapped_rdd.groupByKey()\n",
    "word_count = grouped_rdd.mapValues(sum).map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "print(word_count.take(10))\n",
    "\n",
    "# Distinct()\n",
    "print(\"\\nUnique Word count without stop words - \",flatten_array.distinct().count())\n",
    "print(\"\\nUnique Word count after removing stop words - \",filtered_words.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 4)), ('b', (10, 20))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a',2),('b',10)])\n",
    "rdd2 = sc.parallelize([('a',4),('b',20),('c',30)])\n",
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.csv(\"titanic/train.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Survived|   Sex|\n",
      "+--------+------+\n",
      "|       0|  male|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       1|female|\n",
      "|       0|  male|\n",
      "|       0|  male|\n",
      "|       0|female|\n",
      "|       1|female|\n",
      "|       0|  male|\n",
      "|       1|  male|\n",
      "|       0|female|\n",
      "|       1|female|\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.select('Survived','Sex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|               714|\n",
      "|   mean| 29.69911764705882|\n",
      "| stddev|14.526497332334035|\n",
      "|    min|              0.42|\n",
      "|    max|              80.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|    Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-----+--------+\n",
      "|         31|       0|     1|Uruchurtu, Don. M...|  male|40.0|    0|    0|  PC 17601| 27.7208| null|       C|\n",
      "|         41|       0|     3|Ahlin, Mrs. Johan...|female|40.0|    1|    0|      7546|   9.475| null|       S|\n",
      "|        162|       1|     2|\"Watt, Mrs. James...|female|40.0|    0|    0|C.A. 33595|   15.75| null|       S|\n",
      "|        189|       0|     3|    Bourke, Mr. John|  male|40.0|    1|    1|    364849|    15.5| null|       Q|\n",
      "|        210|       1|     1|    Blank, Mr. Henry|  male|40.0|    0|    0|    112277|    31.0|  A31|       C|\n",
      "|        264|       0|     1|Harrison, Mr. Wil...|  male|40.0|    0|    0|    112059|     0.0|  B94|       S|\n",
      "|        320|       1|     1|Spedden, Mrs. Fre...|female|40.0|    1|    1|     16966|   134.5|  E34|       C|\n",
      "|        347|       1|     2|Smith, Miss. Mari...|female|40.0|    0|    0|     31418|    13.0| null|       S|\n",
      "|        361|       0|     3|  Skoog, Mr. Wilhelm|  male|40.0|    1|    4|    347088|    27.9| null|       S|\n",
      "|        562|       0|     3|   Sivic, Mr. Husein|  male|40.0|    0|    0|    349251|  7.8958| null|       S|\n",
      "|        610|       1|     1|Shutes, Miss. Eli...|female|40.0|    0|    0|  PC 17582|153.4625| C125|       S|\n",
      "|        662|       0|     3|   Badt, Mr. Mohamed|  male|40.0|    0|    0|      2623|   7.225| null|       C|\n",
      "|        671|       1|     2|Brown, Mrs. Thoma...|female|40.0|    1|    1|     29750|    39.0| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.filter(titanic_df.Age==40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|    Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|  113783|   26.55| C103|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|  248706|    16.0| null|       S|\n",
      "|         53|       1|     1|Harper, Mrs. Henr...|female|49.0|    1|    0|PC 17572| 76.7292|  D33|       C|\n",
      "|        188|       1|     1|\"Romaine, Mr. Cha...|  male|45.0|    0|    0|  111428|   26.55| null|       S|\n",
      "|        195|       1|     1|Brown, Mrs. James...|female|44.0|    0|    0|PC 17610| 27.7208|   B4|       C|\n",
      "|        196|       1|     1|Lurette, Miss. Elise|female|58.0|    0|    0|PC 17569|146.5208|  B80|       C|\n",
      "|        260|       1|     2|Parrish, Mrs. (Lu...|female|50.0|    0|    1|  230433|    26.0| null|       S|\n",
      "|        269|       1|     1|Graham, Mrs. Will...|female|58.0|    0|    1|PC 17582|153.4625| C125|       S|\n",
      "|        273|       1|     2|Mellinger, Mrs. (...|female|41.0|    0|    1|  250644|    19.5| null|       S|\n",
      "|        276|       1|     1|Andrews, Miss. Ko...|female|63.0|    1|    0|   13502| 77.9583|   D7|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.where((titanic_df.Age > 40) & (titanic_df.Survived==1)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating temp table, so that we can query like SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the df to table with the name titanic_sql\n",
    "titanic_df.registerTempTable(\"titanic_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from titanic_sql\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|    Fare|  Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-------+--------+\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|    113783|   26.55|   C103|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|    248706|    16.0|   null|       S|\n",
      "|         53|       1|     1|Harper, Mrs. Henr...|female|49.0|    1|    0|  PC 17572| 76.7292|    D33|       C|\n",
      "|        162|       1|     2|\"Watt, Mrs. James...|female|40.0|    0|    0|C.A. 33595|   15.75|   null|       S|\n",
      "|        188|       1|     1|\"Romaine, Mr. Cha...|  male|45.0|    0|    0|    111428|   26.55|   null|       S|\n",
      "|        195|       1|     1|Brown, Mrs. James...|female|44.0|    0|    0|  PC 17610| 27.7208|     B4|       C|\n",
      "|        196|       1|     1|Lurette, Miss. Elise|female|58.0|    0|    0|  PC 17569|146.5208|    B80|       C|\n",
      "|        210|       1|     1|    Blank, Mr. Henry|  male|40.0|    0|    0|    112277|    31.0|    A31|       C|\n",
      "|        260|       1|     2|Parrish, Mrs. (Lu...|female|50.0|    0|    1|    230433|    26.0|   null|       S|\n",
      "|        269|       1|     1|Graham, Mrs. Will...|female|58.0|    0|    1|  PC 17582|153.4625|   C125|       S|\n",
      "|        273|       1|     2|Mellinger, Mrs. (...|female|41.0|    0|    1|    250644|    19.5|   null|       S|\n",
      "|        276|       1|     1|Andrews, Miss. Ko...|female|63.0|    1|    0|     13502| 77.9583|     D7|       S|\n",
      "|        289|       1|     2|Hosono, Mr. Masabumi|  male|42.0|    0|    0|    237798|    13.0|   null|       S|\n",
      "|        300|       1|     1|Baxter, Mrs. Jame...|female|50.0|    0|    1|  PC 17558|247.5208|B58 B60|       C|\n",
      "|        320|       1|     1|Spedden, Mrs. Fre...|female|40.0|    1|    1|     16966|   134.5|    E34|       C|\n",
      "|        338|       1|     1|Burns, Miss. Eliz...|female|41.0|    0|    0|     16966|   134.5|    E40|       C|\n",
      "|        339|       1|     3|Dahl, Mr. Karl Ed...|  male|45.0|    0|    0|      7598|    8.05|   null|       S|\n",
      "|        347|       1|     2|Smith, Miss. Mari...|female|40.0|    0|    0|     31418|    13.0|   null|       S|\n",
      "|        367|       1|     1|Warren, Mrs. Fran...|female|60.0|    1|    0|    110813|   75.25|    D37|       C|\n",
      "|        381|       1|     1|Bidois, Miss. Ros...|female|42.0|    0|    0|  PC 17757| 227.525|   null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+--------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from titanic_sql where Age >= 40 and Survived=1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      61|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(*) from titanic_sql where Age >= 40 and Survived=1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to postgresql db and reading/writing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - Launched pyspark with below command\n",
    "# pyspark --driver-class-path .\\postgresql-42.2.18.jar --jars .\\postgresql-42.2.18.jar\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', \"postgresql-42.2.18.jar\").getOrCreate()\n",
    "url = 'jdbc:postgresql://127.0.0.1/postgres'\n",
    "properties = {'user': 'postgres', 'password': 'n0ob007'}\n",
    "df = spark.read.jdbc(url=url, table='active_users', properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+----------+-----+------+\n",
      "|user_id|country|     platform|  date_utc|years|months|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "|      1|Android|           PE|01-07-2020| 2020|     7|\n",
      "|      2|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      3|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      4|Android|           PT|01-07-2020| 2020|     7|\n",
      "|      5|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      6|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      7|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      8|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|      9|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     10|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     11|Android|           US|01-07-2020| 2020|     7|\n",
      "|     12|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     13|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     14|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     15|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     16|Android|           US|01-07-2020| 2020|     7|\n",
      "|     17|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     18|Android|           US|01-07-2020| 2020|     7|\n",
      "|     19|    iOS|United States|01-07-2020| 2020|     7|\n",
      "|     20|    iOS|United States|01-07-2020| 2020|     7|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- date_utc: string (nullable = true)\n",
      " |-- years: integer (nullable = true)\n",
      " |-- months: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SparkSession'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ef52b3418092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicits\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m a = spark.createDataset(\"\"\"{\"user_id\":150,\n\u001b[0;32m      3\u001b[0m        \u001b[1;34m\"country\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"Android\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m        \u001b[1;34m\"platform\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"United States\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;34m\"date_utc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"01-07-2020\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SparkSession'"
     ]
    }
   ],
   "source": [
    "from SparkSession.implicits import *\n",
    "a = spark.createDataset(\"\"\"{\"user_id\":150,\n",
    "       \"country\":\"Android\",\n",
    "       \"platform\":\"United States\",\n",
    "       \"date_utc\":\"01-07-2020\",\n",
    "       \"years\":2020,\n",
    "       \"months\":2}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_df = spark.read.json(\"test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_df.registerTempTable(\"json_data_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_data_sql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-15dcb0406103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjson_data_sql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json_data_sql' is not defined"
     ]
    }
   ],
   "source": [
    "json_data_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframe in pandas and converting it to pyspark dataframe and insert it to postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'user_id':[150],\n",
    "       'country':[\"Android\"],\n",
    "       'platform':['United States'],\n",
    "       'date_utc':['01-07-2020'],\n",
    "       'years':[2020],\n",
    "       'months':[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+----------+-----+------+\n",
      "|user_id|country|     platform|  date_utc|years|months|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "|    150|Android|United States|01-07-2020| 2020|     2|\n",
      "+-------+-------+-------------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pyspark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df.write\\\n",
    "        .format(\"jdbc\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"truncate\",\"true\")\\\n",
    "        .option(\"url\",url)\\\n",
    "        .option(\"dbtable\",\"active_users\")\\\n",
    "        .option(\"user\",\"postgres\")\\\n",
    "        .option(\"password\",\"n0ob007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:postgresql://127.0.0.1/postgres'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
